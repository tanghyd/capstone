{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Visualization - T-SNE and Doc2Vec\n",
    "Source: https://www.kaggle.com/arthurtok/target-visualization-t-sne-and-doc2vec/notebook\n",
    "\n",
    "This kernel will be an exploration into the target variable and how it is distributed accorss the structure of the training data to see if any potential information or patterns can be gleaned going forwards. Since classical treatment of text data normally comes with the challenges of high dimensionality (using terms frequencies or term frequency inverse document frequencies), the plan therefore in this kernel is to visually explore the target variable in some lower dimensional space using SVD and LSA(Latent Semantic Analysis) and Doc2Vec method. In these lower dimensional spaces, we can finally utilize the manifold learning method of the t-distributed stochastic neighbour embedding (tNSE) technique to further reduce the dimensionality for target variable visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import re\n",
    "from functools import reduce\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook, reset_output\n",
    "from bokeh.palettes import d3\n",
    "import bokeh.models as bmo\n",
    "from bokeh.io import save, output_file\n",
    "\n",
    "# init_notebook_mode(connected = True)\n",
    "# color = sns.color_palette(\"Set2\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path('..')\n",
    "events_path = BASE_PATH / 'events'\n",
    "dictionary_path = BASE_PATH / 'dictionary'\n",
    "data_path = BASE_PATH / 'data'\n",
    "subset_reports_path = data_path / 'subset'\n",
    "subset_reports_path_txt = data_path / 'subset_txt'\n",
    "df_path = data_path / 'dataframes'\n",
    "patterns_path = dictionary_path / 'patterns'\n",
    "triggers_path = dictionary_path / 'trigger phrases'\n",
    "\n",
    "\n",
    "#group_events_path = events_path / f'group_{GROUP}_events.csv'\n",
    "#labelled_path = events_path / f'group_{GROUP}_labelled.csv'\n",
    "#processed_path = events_path / f'group_{GROUP}_processed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP packages \n",
    "import string\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['False']).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: WindowsPath('../events/group_0_labelled.csv'), 1: WindowsPath('../events/group_1_labelled.csv'), 2: WindowsPath('../events/group_2_labelled.csv'), 3: WindowsPath('../events/group_3_labelled.csv'), 4: WindowsPath('../events/group_4_labelled.csv'), 6: WindowsPath('../events/group_6_labelled.csv')}\n"
     ]
    }
   ],
   "source": [
    "groups = [0, 1, 2, 3, 4, 6] # lol\n",
    "\n",
    "filenames = {group: events_path / f'group_{group}_labelled.csv' for group in groups}\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'group_6_labelled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d1025fbd35b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_6_labelled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'group_6_labelled' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(group_6_labelled.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ..\\events\\group_0_labelled.csv does not exist: '..\\\\events\\\\group_0_labelled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0868452a5977>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdfall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# data processing and cleaning on near miss event column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\CITS5508\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\CITS5508\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\CITS5508\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\CITS5508\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\CITS5508\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ..\\events\\group_0_labelled.csv does not exist: '..\\\\events\\\\group_0_labelled.csv'"
     ]
    }
   ],
   "source": [
    "#Read in labelled event data file from 6 groups\n",
    "\n",
    "groups = [0, 1, 2, 3, 4, 6] # lol\n",
    "\n",
    "filenames = {group: events_path / f'group_{group}_labelled.csv' for group in groups}\n",
    "\n",
    "# instantiate empty list to store dfs on read\n",
    "dfall = []\n",
    "for group in groups:\n",
    "    df = pd.read_csv(filenames[group])\n",
    "    \n",
    "    # data processing and cleaning on near miss event column\n",
    "    df = df.loc[df['Near Miss Event'].notna(), ]\n",
    "    \n",
    "    # pd.Series(['False']) returns True as string are converted to bool on whether they are empty or not!\n",
    "    df['Near Miss Event'] = df['Near Miss Event'].apply(lambda x : (x == 'True') | (x == True)).astype(bool)\n",
    "    \n",
    "    # need to read in dataframe to work out length of group column\n",
    "    df.insert(2, 'group', np.repeat(group, len(df)))\n",
    "    dfall.append(df)\n",
    "    \n",
    "# concat list of dfs as a single data frame containing all labelled events from 6 groups\n",
    "dfall = pd.concat(dfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall.to_csv(events_path / f'group_all_labelled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfall.loc[dfall.reviewed][['event_id','filename', 'group', 'sentence_text','event_text', 'Near Miss Event']]\n",
    "# Target Label\n",
    "df['Label'] = df['Near Miss Event'].astype(int)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply standard NLP steps to process the event text from the input file, including:\n",
    "\n",
    "* Removing stop words \n",
    "\n",
    "* Tokenization\n",
    "\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Spacy functions\n",
    "df[\"tokenized_text\"] = df[\"event_text\"].apply(lambda x: spacy_tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. T-SNE applied to Latent Semantic (LSA) space\n",
    "\n",
    "\n",
    "To start off we look at the sparse representation of text documents via the Term frequency Inverse document frequency method. What this does is create a matrix representation that upweights locally prevalent but globally rare terms - therefore accounting for the occurence bias when using just term frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much data comes from each subgroup?\n",
    "df.group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(min_df=3,\n",
    "                             max_features = 60_000, #100_000,\n",
    "                             analyzer=\"word\",\n",
    "                             ngram_range=(1,3), # (1,6)\n",
    "                             stop_words=\"english\")\n",
    "\n",
    "# fit and transform on all events\n",
    "tf_idf = tf_idf_vec.fit_transform(list(df[\"tokenized_text\"].map(lambda tokens: \" \".join(tokens))))\n",
    "\n",
    "# fit on all events, transform subset\n",
    "# tf_idf_vec.fit(list(X[\"event_text\"].map(lambda tokens: \" \".join(tokens))))\n",
    "# tf_idf = tf_idf_vec.transform(list(X.loc[X.group == GROUP,\"event_text\"].map(lambda tokens: \" \".join(tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Singular value decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50, random_state=2018)\n",
    "svd_tfidf = svd.fit_transform(tf_idf)\n",
    "print(\"Dimensionality of LSA space: {}\".format(svd_tfidf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Showing scatter plots \n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# fig = plt.figure(figsize=(16,12))\n",
    "\n",
    "# # Plot models:\n",
    "# ax = Axes3D(fig) \n",
    "# ax.scatter(svd_tfidf[:,0],\n",
    "#            svd_tfidf[:,1],\n",
    "#            svd_tfidf[:,2],\n",
    "#            c=X.Label.values,\n",
    "#            cmap=plt.cm.winter_r,\n",
    "#            s=2,\n",
    "#            edgecolor='none',\n",
    "#            marker='o')\n",
    "# plt.title(\"Semantic Tf-Idf-SVD reduced plot of Sincere-Insincere data distribution\")\n",
    "# plt.xlabel(\"First dimension\")\n",
    "# plt.ylabel(\"Second dimension\")\n",
    "# plt.legend()\n",
    "# plt.xlim(0.0, 0.20)\n",
    "# plt.ylim(-0.2,0.4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Importing multicore version of TSNE\n",
    "#from MulticoreTSNE import MulticoreTSNE as TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_jobs=4,\n",
    "                  early_exaggeration=4, # Trying out exaggeration trick\n",
    "                  n_components=2,\n",
    "                  verbose=1,\n",
    "                  random_state=2018,\n",
    "                  n_iter=500)\n",
    "\n",
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)\n",
    "\n",
    "# Putting the tsne information into a dataframe\n",
    "tsne_tfidf_df = pd.DataFrame(data=tsne_tfidf, columns=[\"x\", \"y\"])\n",
    "\n",
    "# add X values to full df\n",
    "for col in ['event_id','filename', 'sentence_text', 'event_text', 'tokenized_text', 'Label', 'group']:\n",
    "    tsne_tfidf_df[col] = df[col].values\n",
    "\n",
    "# add X values to subsetted df (i.e. for only one GROUP)\n",
    "# for col in ['filename', 'sentence_text', 'event_text', 'Label', 'group']:\n",
    "#     tsne_tfidf_df[col] = X[X.loc[X.group == GROUP, col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\n",
    "\n",
    "# we need a list of length 7 becasue charlie labelled group 6 instead of 5 lol\n",
    "colormap = np.array([\"darkblue\", \"red\", \"purple\", \"green\", \"orange\", \"yellow\", \"yellow\"])\n",
    "\n",
    "# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\n",
    "source = ColumnDataSource(data = dict(x = tsne_tfidf_df[\"x\"], \n",
    "                                      y = tsne_tfidf_df[\"y\"],\n",
    "                                      color = colormap[tsne_tfidf_df[\"Label\"]],\n",
    "                                      group = tsne_tfidf_df[\"group\"],\n",
    "                                      sentence_text = tsne_tfidf_df[\"sentence_text\"],\n",
    "                                      event_text = tsne_tfidf_df[\"event_text\"],\n",
    "                                      filename = tsne_tfidf_df[\"filename\"],\n",
    "                                      event_id = tsne_tfidf_df[\"event_id\"],\n",
    "                                      Label = tsne_tfidf_df[\"Label\"]))\n",
    "TOOLTIPS = [\n",
    "    (\"event_id\",\"@event_id\"),\n",
    "    (\"filename\", \"@filename\"),\n",
    "    (\"event_text\", \"@event_text\"),\n",
    "    (\"Label\",\"@Label\"), \n",
    "    (\"group\", \"@group\")\n",
    "]\n",
    "\n",
    "\n",
    "plot_tfidf = bp.figure(plot_width = 800, plot_height = 700, tooltips=TOOLTIPS,\n",
    "                       title = \"T-SNE applied to Tfidf_SVD space\")\n",
    "\n",
    "plot_tfidf.scatter(x = \"x\", \n",
    "                   y = \"y\", \n",
    "                   color=\"color\",\n",
    "                   legend = \"Label\",\n",
    "                   source = source,\n",
    "                   alpha = 0.7,\n",
    "                   radius = 0.4)\n",
    "\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\n",
    "\n",
    "# we need a list of length 7 becasue charlie labelled group 6 instead of 5 lol\n",
    "colormap = np.array([\"darkblue\", \"red\", \"purple\", \"green\", \"orange\", \"yellow\", \"yellow\"])\n",
    "\n",
    "# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\n",
    "source = ColumnDataSource(data = dict(x = tsne_tfidf_df[\"x\"], \n",
    "                                      y = tsne_tfidf_df[\"y\"],\n",
    "                                      color = colormap[tsne_tfidf_df[\"group\"]],\n",
    "                                      group = tsne_tfidf_df[\"group\"],\n",
    "                                      sentence_text = tsne_tfidf_df[\"sentence_text\"],\n",
    "                                      event_text = tsne_tfidf_df[\"event_text\"],\n",
    "                                      event_id = tsne_tfidf_df[\"event_id\"],\n",
    "                                      filename = tsne_tfidf_df[\"filename\"],\n",
    "                                      Label = tsne_tfidf_df[\"Label\"]))\n",
    "TOOLTIPS = [\n",
    "     (\"event_id\",\"@event_id\"),\n",
    "    (\"filename\", \"@filename\"),\n",
    "#    (\"sentence_text\", \"@sentence_text\"),  # show centre sentence of text chunk\n",
    "    (\"event_text\", \"@event_text\"), # show full text chunk\n",
    "    (\"Label\",\"@Label\"),\n",
    "    (\"group\", \"@group\")\n",
    "]\n",
    "\n",
    "\n",
    "plot_tfidf = bp.figure(plot_width = 800, plot_height = 700, tooltips=TOOLTIPS,\n",
    "                       title = \"T-SNE applied to Tfidf_SVD space\")\n",
    "\n",
    "plot_tfidf.scatter(x = \"x\", \n",
    "                   y = \"y\", \n",
    "                   color=\"color\",\n",
    "                   legend = \"group\",\n",
    "                   source = source,\n",
    "                   alpha = 0.7,\n",
    "                   radius = 0.35)  # adjust scatter point size\n",
    "\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. T-SNE applied on Doc2Vec embedding\n",
    "Pushing forward with our T-SNE visual explorations, we next move away from semantic matrices into the realm of embeddings. Here we will use the Doc2Vec algorithm and much like its very well known counterpart Word2vec involves unsupervised learning of continuous representations for text. Unlike Word2vec which involves finding the representations for words (i.e. word embeddings), Doc2vec modifies the former method and extends it to sentences and even documents.\n",
    "\n",
    "For this notebook, we will be using gensim's Doc2Vec class which inherits from the base Word2Vec class where style of usage and parameters are similar. The only differences lie in the naming terminology of the training method used which are the “distributed memory” or “distributed bag of words” methods.\n",
    "\n",
    "According to the Gensim documentation, Doc2Vec requires the input to be an iterable object representing the sentences in the form of two lists, a list of the terms and a list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the question texts in a list\n",
    "event_texts = list(df[\"event_text\"])\n",
    "\n",
    "# Creating a list of terms and a list of labels to go with it\n",
    "documents = [TaggedDocument(doc, tags=[str(i)]) for i, doc in enumerate(event_texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement Doc2Vec\n",
    "max_epochs = 100\n",
    "alpha=0.025\n",
    "model = Doc2Vec(documents,\n",
    "                size=10, \n",
    "                min_alpha=0.00025,\n",
    "                alpha=alpha,\n",
    "                min_count=1,\n",
    "#                 window=2, \n",
    "                workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the tsne model to the document embeddings\n",
    "tsne_model = TSNE(n_jobs=4,\n",
    "                  early_exaggeration=4,\n",
    "                  n_components=2,\n",
    "                  verbose=1,\n",
    "                  random_state=2018,\n",
    "                  n_iter=300)\n",
    "\n",
    "# fit alll\n",
    "#tsne_d2v = tsne_model.fit_transform(model.docvecs.vectors_docs)\n",
    "\n",
    "tsne_d2v = tsne_model.fit_transform(model.docvecs.vectors_docs)\n",
    "\n",
    "# Putting the tsne information into sq\n",
    "tsne_d2v_df = pd.DataFrame(data=tsne_d2v, columns=[\"x\", \"y\"])\n",
    "\n",
    "# add X values to full df\n",
    "for col in ['event_id','filename', 'sentence_text', 'event_text', 'tokenized_text', 'Label', 'group']:\n",
    "    tsne_d2v_df[col] = df[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\n",
    "colormap = np.array([\"darkblue\", \"red\", \"purple\", \"green\", \"orange\", \"yellow\", \"yellow\"])\n",
    "\n",
    "# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\n",
    "source = ColumnDataSource(data = dict(x = tsne_d2v_df[\"x\"], \n",
    "                                      y = tsne_d2v_df[\"y\"],\n",
    "                                      color = colormap[tsne_d2v_df[\"Label\"]],\n",
    "                                      group = tsne_d2v_df[\"group\"],\n",
    "                                      event_text = tsne_d2v_df[\"event_text\"],\n",
    "                                      sentence_text = tsne_d2v_df['sentence_text'],\n",
    "                                      event_id = tsne_d2v_df[\"event_id\"],\n",
    "                                      filename = tsne_d2v_df[\"filename\"],\n",
    "                                      Label = tsne_d2v_df[\"Label\"]))\n",
    "\n",
    "TOOLTIPS = [\n",
    "     (\"event_id\",\"@event_id\"),\n",
    "    (\"filename\", \"@filename\"),\n",
    "    (\"sentence_text\", \"@sentence_text\"),\n",
    "    (\"event_text\", \"@event_text\"),\n",
    "    (\"Label\",\"@Label\"),\n",
    "    (\"group\", \"@group\")\n",
    "]\n",
    "\n",
    "plot_d2v = bp.figure(plot_width = 800, plot_height = 700, tooltips=TOOLTIPS,\n",
    "                       title = \"T-SNE applied to Doc2vec document embeddings\")\n",
    "\n",
    "plot_d2v.scatter(x = \"x\", \n",
    "                   y = \"y\", \n",
    "                   color=\"color\",\n",
    "                   legend = \"Label\",\n",
    "                   source = source,\n",
    "                   alpha = 0.7,\n",
    "                   radius = 0.15)\n",
    "\n",
    "show(plot_d2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways from the plot\n",
    "\n",
    "The visual overlap between near miss and non near miss event are even greater in the Doc2Vec plots - so much so that there doesn't seem to be any obvious manner to segragate the labels via eye-balling if going down the route of document embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
