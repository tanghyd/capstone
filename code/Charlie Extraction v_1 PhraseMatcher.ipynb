{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1\n",
    "\n",
    "Prepare the dataset (events.csv)\n",
    "- Ectract event-related text chuncks from the reports. A text chunck can be less than 100 words around a trigger phrase.\n",
    "- E.g. When the trigger phrase is detected, 9 sentences are extracted, 4 sentences in front + the trigger sentence + 4 sentences after.\n",
    "- Save the text chucks with 3 columns: Document_ID, Text_Chunck and Document_Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "import os, glob #glob to open file folder\n",
    "import csv #to read and write csv file \n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv (r'C:\\Users\\charl\\OneDrive - The University of Western Australia\\Master DATA SCIENCE\\2020\\SEMESTER 2 2020\\CITS5553 Data Science Capstone\\report_groups.csv') \n",
    "charlie_files = df[df[\"group\"] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test report\n",
    "# Opening JSON file\n",
    "report = \"C:/Users/charl/OneDrive - The University of Western Australia/Master DATA SCIENCE/2020/SEMESTER 2 2020/CITS5553 Data Science Capstone/data/wamex_xml/a071725_comannualrepc65-2005_draft2_13774268.json\"\n",
    "f = open(report,'r')  \n",
    "report_sentences = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/charl/OneDrive - The University of Western Australia/Master DATA SCIENCE/2020/SEMESTER 2 2020/CITS5553 Data Science Capstone/\"\n",
    "\n",
    "def get_report(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        report_sentences = json.load(f)\n",
    "        f.close()\n",
    "        return report_sentences\n",
    "    \n",
    "# To begin with, we are just going to use the list of near-miss phrases provided by Paul as our trigger phrases\n",
    "near_miss = \"C:/Users/charl/OneDrive - The University of Western Australia/Master DATA SCIENCE/2020/SEMESTER 2 2020/CITS5553 Data Science Capstone/Near_miss_terms.txt\"\n",
    "near_miss_phrases = []\n",
    "\n",
    "temp = open(near_miss,'r')\n",
    "with temp as myfile:\n",
    "    for line in myfile:\n",
    "        if len(line) > 1:\n",
    "            near_miss_phrases.append(line[:-2].split())\n",
    "temp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets go through the sentences in the report and apply a cleaning function and store those sentences in a list\n",
    "## Cleaning Function\n",
    "def clean(text):\n",
    "    text = text.strip('[(),- :\\'\\\"\\n]\\s*').lower()\n",
    "    #text = re.sub('([A-Za-z0-9\\)]{2,}\\.)([A-Z]+[a-z]*)', r\"\\g<1> \\g<2>\", text, flags=re.UNICODE)\n",
    "    text = re.sub('\\s+', ' ', text, flags=re.UNICODE).strip()\n",
    "    text = re.sub('\",\"', ' ', text, flags=re.UNICODE).strip()\n",
    "    text = re.sub('-', ' ', text, flags=re.UNICODE).strip()\n",
    "    #text = re.sub('\\(', ' ', text, flags=re.UNICODE).strip()\n",
    "    #text = re.sub('\\)', ' ', text, flags=re.UNICODE).strip()\n",
    "    text = re.sub('\\/', ' ', text, flags=re.UNICODE).strip()\n",
    "    text = text.replace(\"\\\\\", ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    if (text[len(text)-1] != '.'):\n",
    "        text += '.'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create phrase matcher object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "near_phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "near_miss_phrases_string = []\n",
    "for i in near_miss_phrases:\n",
    "    near_miss_phrases_string.append(' '.join(i))\n",
    "\n",
    "patterns = [nlp(text) for text in near_miss_phrases_string]\n",
    "\n",
    "near_phrase_matcher.add(\"NearMissEvent\",None, *patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_report_sentences = []\n",
    "for sentence in report_sentences:\n",
    "    clean_sentence = clean(sentence)\n",
    "    cleaned_report_sentences.append(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15890475404117588905 NearMissEvent 4643 4644 encouraging\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(str(cleaned_report_sentences))\n",
    "matched_phrases = near_phrase_matcher(doc)\n",
    "for match_id, start, end in matched_phrases:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = doc[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "margin of the lsb.', 'no fresh gravels presently come down the lower ord river system as far as the lsb.', 'there is no evidence of modern rechanging of gravels.', 'confidential page 20 5.', 'tidal currents prevent fine sediment from settling in the southern half of the lsb, but are not re working coarse sediments.', 'the fluvial gravels are a plausible host for a marine diamond deposit.', 'a lack of diamond recoveries, although not very encouraging, need not indicate a lack of a diamond deposit in the prospect.', 'a longer sampling programme will be required for fully testing the potential of the area.', 'the northern 25% of the lsb covers an area of approximately 9km2 with less than 4m overburden.', 'this area has not yet been sampled.', 'the northern lsb contains the most substantial areas of fluvial gravel and as such appears to be the best potential diamond target for continued exploration.', 'looking"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"Document_ID\", \"Text_Chunk\", \"Trigger_words_activated\", \"Classification_vocab\"]\n",
    "event_extraction = pd.DataFrame(columns = column_names)\n",
    "\n",
    "doc = nlp(str(cleaned_report_sentences))\n",
    "matched_phrases = near_phrase_matcher(doc)\n",
    "\n",
    "for match_id, start, end in matched_phrases:\n",
    "     event_extraction_row = {\"Document_ID\":\"Sample\",\n",
    "                             \"Text_Chunk\": doc[start-100:end+100],\n",
    "                             \"Trigger_words_activated\": span.text,\n",
    "                             \"Classification_vocab\": string_id}\n",
    "     event_extraction = event_extraction.append(event_extraction_row, ignore_index=True)\n",
    "    \n",
    "event_extraction[\"Text_Chunk\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create phrase matcher object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "near_phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "near_miss_phrases_string = []\n",
    "for i in near_miss_phrases:\n",
    "    near_miss_phrases_string.append(' '.join(i))\n",
    "\n",
    "patterns = [nlp(text) for text in near_miss_phrases_string]\n",
    "\n",
    "near_phrase_matcher.add(\"NearMissEvent\",None, *patterns)\n",
    "column_names = [\"Document_ID\",\"Text_Chunk_ID\", \"Text_Chunk\", \"Trigger_words_activated\", \"Classification_vocab\"]\n",
    "event_extraction = pd.DataFrame(columns = column_names)\n",
    "\n",
    "\n",
    "for report_path, report_name in zip(charlie_files[\"path\"],charlie_files['name']):\n",
    "    file_path = folder_path + str(report_path)\n",
    "    report_content = get_report(file_path)\n",
    "    \n",
    "    cleaned_report_sentences = []\n",
    "    for sentence in report_content:\n",
    "        clean_sentence = clean(sentence)\n",
    "        cleaned_report_sentences.append(clean_sentence)\n",
    "    \n",
    "    doc = nlp(str(cleaned_report_sentences))\n",
    "    matched_phrases = near_phrase_matcher(doc)\n",
    "    for match_id, start, end in matched_phrases:\n",
    "        count = 1\n",
    "        event_extraction_row = {\"Document_ID\": str(report_name),\n",
    "                                \"Text_Chunk_ID\": str(report_name + 'Chunk'+ str(count)),\n",
    "                                \"Text_Chunk\": doc[start-100:end+100],\n",
    "                                \"Trigger_words_activated\": doc[start:end],\n",
    "                                \"Classification_vocab\": string_id}\n",
    "        event_extraction = event_extraction.append(event_extraction_row, ignore_index=True)\n",
    "        count +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_ID</th>\n",
       "      <th>Text_Chunk_ID</th>\n",
       "      <th>Text_Chunk</th>\n",
       "      <th>Trigger_words_activated</th>\n",
       "      <th>Classification_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a074375_annual report m59</td>\n",
       "      <td>a074375_annual report m59Chunk1</td>\n",
       "      <td>(lake, moore, at, the, southern, end, of, a, m...</td>\n",
       "      <td>(broad)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a074375_annual report m59</td>\n",
       "      <td>a074375_annual report m59Chunk1</td>\n",
       "      <td>(than, the, back, dune, ., ', ,, ', photograph...</td>\n",
       "      <td>(broad)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a074375_annual report m59</td>\n",
       "      <td>a074375_annual report m59Chunk1</td>\n",
       "      <td>(inferred, resource, for, all, areas, of, 80mt...</td>\n",
       "      <td>(rising)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a078309_weebacarry 2007-2008 annual report_146...</td>\n",
       "      <td>a078309_weebacarry 2007-2008 annual report_146...</td>\n",
       "      <td>(', a, summary, of, the, historical, activity,...</td>\n",
       "      <td>(extensive)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a078309_weebacarry 2007-2008 annual report_146...</td>\n",
       "      <td>a078309_weebacarry 2007-2008 annual report_146...</td>\n",
       "      <td>(foliated, ., ', ,, ', interestingly, the, str...</td>\n",
       "      <td>(broad)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>a081220_yn_e53_1156_part_surrender_rpt_2008_12...</td>\n",
       "      <td>a081220_yn_e53_1156_part_surrender_rpt_2008_12...</td>\n",
       "      <td>(mckay, and, miezitis, ,, 2001, ), ., ', ,, ',...</td>\n",
       "      <td>(extensive)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>a081220_yn_e53_1156_part_surrender_rpt_2008_12...</td>\n",
       "      <td>a081220_yn_e53_1156_part_surrender_rpt_2008_12...</td>\n",
       "      <td>(below, the, water, table, ., ', ,, ', mineral...</td>\n",
       "      <td>(extensive)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>a081566_c21_2007_annual 2008_cr34109_ver_10415274</td>\n",
       "      <td>a081566_c21_2007_annual 2008_cr34109_ver_10415...</td>\n",
       "      <td>(c21, 2007, woodline, project, annual, report,...</td>\n",
       "      <td>(broad)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>a081566_c21_2007_annual 2008_cr34109_ver_10415274</td>\n",
       "      <td>a081566_c21_2007_annual 2008_cr34109_ver_10415...</td>\n",
       "      <td>(', ,, ', follow, up, rab, drilling, along, st...</td>\n",
       "      <td>(encouraging)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>a081566_c21_2007_annual 2008_cr34109_ver_10415274</td>\n",
       "      <td>a081566_c21_2007_annual 2008_cr34109_ver_10415...</td>\n",
       "      <td>(storey, of, saltbush, bluebush, ., ', ,, ', l...</td>\n",
       "      <td>(broad)</td>\n",
       "      <td>NearMissEvent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Document_ID  \\\n",
       "0                            a074375_annual report m59   \n",
       "1                            a074375_annual report m59   \n",
       "2                            a074375_annual report m59   \n",
       "3    a078309_weebacarry 2007-2008 annual report_146...   \n",
       "4    a078309_weebacarry 2007-2008 annual report_146...   \n",
       "..                                                 ...   \n",
       "267  a081220_yn_e53_1156_part_surrender_rpt_2008_12...   \n",
       "268  a081220_yn_e53_1156_part_surrender_rpt_2008_12...   \n",
       "269  a081566_c21_2007_annual 2008_cr34109_ver_10415274   \n",
       "270  a081566_c21_2007_annual 2008_cr34109_ver_10415274   \n",
       "271  a081566_c21_2007_annual 2008_cr34109_ver_10415274   \n",
       "\n",
       "                                         Text_Chunk_ID  \\\n",
       "0                      a074375_annual report m59Chunk1   \n",
       "1                      a074375_annual report m59Chunk1   \n",
       "2                      a074375_annual report m59Chunk1   \n",
       "3    a078309_weebacarry 2007-2008 annual report_146...   \n",
       "4    a078309_weebacarry 2007-2008 annual report_146...   \n",
       "..                                                 ...   \n",
       "267  a081220_yn_e53_1156_part_surrender_rpt_2008_12...   \n",
       "268  a081220_yn_e53_1156_part_surrender_rpt_2008_12...   \n",
       "269  a081566_c21_2007_annual 2008_cr34109_ver_10415...   \n",
       "270  a081566_c21_2007_annual 2008_cr34109_ver_10415...   \n",
       "271  a081566_c21_2007_annual 2008_cr34109_ver_10415...   \n",
       "\n",
       "                                            Text_Chunk  \\\n",
       "0    (lake, moore, at, the, southern, end, of, a, m...   \n",
       "1    (than, the, back, dune, ., ', ,, ', photograph...   \n",
       "2    (inferred, resource, for, all, areas, of, 80mt...   \n",
       "3    (', a, summary, of, the, historical, activity,...   \n",
       "4    (foliated, ., ', ,, ', interestingly, the, str...   \n",
       "..                                                 ...   \n",
       "267  (mckay, and, miezitis, ,, 2001, ), ., ', ,, ',...   \n",
       "268  (below, the, water, table, ., ', ,, ', mineral...   \n",
       "269  (c21, 2007, woodline, project, annual, report,...   \n",
       "270  (', ,, ', follow, up, rab, drilling, along, st...   \n",
       "271  (storey, of, saltbush, bluebush, ., ', ,, ', l...   \n",
       "\n",
       "    Trigger_words_activated Classification_vocab  \n",
       "0                   (broad)        NearMissEvent  \n",
       "1                   (broad)        NearMissEvent  \n",
       "2                  (rising)        NearMissEvent  \n",
       "3               (extensive)        NearMissEvent  \n",
       "4                   (broad)        NearMissEvent  \n",
       "..                      ...                  ...  \n",
       "267             (extensive)        NearMissEvent  \n",
       "268             (extensive)        NearMissEvent  \n",
       "269                 (broad)        NearMissEvent  \n",
       "270           (encouraging)        NearMissEvent  \n",
       "271                 (broad)        NearMissEvent  \n",
       "\n",
       "[272 rows x 5 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_extraction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
