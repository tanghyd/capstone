{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sources \n",
    "\n",
    "1. Tutorial: Text Classification in Python Using spaCy: \n",
    "https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/\n",
    "2. A Comprehensive Guide to Understand and Implement Text Classification in Python: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/#:~:text=Introduction,one%20or%20more%20defined%20categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and install necessary packages  \n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for dataset preparation, feature engineering, model training\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import xgboost, textblob, string\n",
    "#from tensorflow import keras\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "\n",
    "#end goal:\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset preparation\n",
    "\n",
    "The first step is the Dataset Preparation step which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>event_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence_idx</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>n_trigger_words</th>\n",
       "      <th>trigger_words_in_sentence</th>\n",
       "      <th>trigger_words_in_event</th>\n",
       "      <th>event_text</th>\n",
       "      <th>ORE_DEPOSIT</th>\n",
       "      <th>ROCK</th>\n",
       "      <th>MINERAL</th>\n",
       "      <th>STRAT</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>TIMESCALE</th>\n",
       "      <th>event_label</th>\n",
       "      <th>reviewed</th>\n",
       "      <th>Near Miss Event</th>\n",
       "      <th>Key trigger phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a081752_anrep2008eraheedy2103_15107355_16</td>\n",
       "      <td>a081752_anrep2008eraheedy2103_15107355.json</td>\n",
       "      <td>16</td>\n",
       "      <td>mineral occurrences and exploration potential ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['potential']</td>\n",
       "      <td>['potential']</td>\n",
       "      <td>bibliography bunting ja 1986, geology of the e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['granite']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>['nabberu basin', 'western australia', 'wester...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a075210_buck_a_ el12_1_2007_11292066_235</td>\n",
       "      <td>a075210_buck_a_ el12_1_2007_11292066.json</td>\n",
       "      <td>235</td>\n",
       "      <td>further drilling in coming years will further ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['further drilling']</td>\n",
       "      <td>['further drilling']</td>\n",
       "      <td>a summary of the coal tonnages within el12 1 i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['coal']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a075210_buck_a_ el12_1_2007_11292066_246</td>\n",
       "      <td>a075210_buck_a_ el12_1_2007_11292066.json</td>\n",
       "      <td>246</td>\n",
       "      <td>the tenement was applied for on the 12 1 2005 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['possible']</td>\n",
       "      <td>['possible']</td>\n",
       "      <td>keywords: ac drilling, diamond core drilling, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ash']</td>\n",
       "      <td>['diamond', 'sulphur']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['muja', 'collie', 'ewington', 'collie']</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a080379_e80_2574_08atr_12876104_4</td>\n",
       "      <td>a080379_e80_2574_08atr_12876104.json</td>\n",
       "      <td>4</td>\n",
       "      <td>the east kimberley halls creek orogen is widel...</td>\n",
       "      <td>2</td>\n",
       "      <td>['potential', 'mineralisation']</td>\n",
       "      <td>['potential', 'mineralisation', 'broad']</td>\n",
       "      <td>if this work is positive drill testing of anom...</td>\n",
       "      <td>['pge']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['gold', 'sulphide']</td>\n",
       "      <td>[]</td>\n",
       "      <td>['kimberley', 'halls creek orogen', 'australia']</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a080379_e80_2574_08atr_12876104_10</td>\n",
       "      <td>a080379_e80_2574_08atr_12876104.json</td>\n",
       "      <td>10</td>\n",
       "      <td>this belt contains the portimo and penikat int...</td>\n",
       "      <td>2</td>\n",
       "      <td>['mineralisation', 'potential']</td>\n",
       "      <td>['mineralisation', 'potential', 'mineralisatio...</td>\n",
       "      <td>the hco has a number of similarities to the to...</td>\n",
       "      <td>['pge', 'pge']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   event_id  \\\n",
       "0           0  a081752_anrep2008eraheedy2103_15107355_16   \n",
       "1           1   a075210_buck_a_ el12_1_2007_11292066_235   \n",
       "2           2   a075210_buck_a_ el12_1_2007_11292066_246   \n",
       "3           3          a080379_e80_2574_08atr_12876104_4   \n",
       "4           4         a080379_e80_2574_08atr_12876104_10   \n",
       "\n",
       "                                      filename  sentence_idx  \\\n",
       "0  a081752_anrep2008eraheedy2103_15107355.json            16   \n",
       "1    a075210_buck_a_ el12_1_2007_11292066.json           235   \n",
       "2    a075210_buck_a_ el12_1_2007_11292066.json           246   \n",
       "3         a080379_e80_2574_08atr_12876104.json             4   \n",
       "4         a080379_e80_2574_08atr_12876104.json            10   \n",
       "\n",
       "                                       sentence_text  n_trigger_words  \\\n",
       "0  mineral occurrences and exploration potential ...                1   \n",
       "1  further drilling in coming years will further ...                1   \n",
       "2  the tenement was applied for on the 12 1 2005 ...                1   \n",
       "3  the east kimberley halls creek orogen is widel...                2   \n",
       "4  this belt contains the portimo and penikat int...                2   \n",
       "\n",
       "         trigger_words_in_sentence  \\\n",
       "0                    ['potential']   \n",
       "1             ['further drilling']   \n",
       "2                     ['possible']   \n",
       "3  ['potential', 'mineralisation']   \n",
       "4  ['mineralisation', 'potential']   \n",
       "\n",
       "                              trigger_words_in_event  \\\n",
       "0                                      ['potential']   \n",
       "1                               ['further drilling']   \n",
       "2                                       ['possible']   \n",
       "3           ['potential', 'mineralisation', 'broad']   \n",
       "4  ['mineralisation', 'potential', 'mineralisatio...   \n",
       "\n",
       "                                          event_text     ORE_DEPOSIT  \\\n",
       "0  bibliography bunting ja 1986, geology of the e...              []   \n",
       "1  a summary of the coal tonnages within el12 1 i...              []   \n",
       "2  keywords: ac drilling, diamond core drilling, ...              []   \n",
       "3  if this work is positive drill testing of anom...         ['pge']   \n",
       "4  the hco has a number of similarities to the to...  ['pge', 'pge']   \n",
       "\n",
       "          ROCK                 MINERAL STRAT  \\\n",
       "0  ['granite']                      []    []   \n",
       "1     ['coal']                      []    []   \n",
       "2      ['ash']  ['diamond', 'sulphur']    []   \n",
       "3           []    ['gold', 'sulphide']    []   \n",
       "4           []                      []    []   \n",
       "\n",
       "                                            LOCATION TIMESCALE  event_label  \\\n",
       "0  ['nabberu basin', 'western australia', 'wester...        []            0   \n",
       "1                                                 []        []            0   \n",
       "2           ['muja', 'collie', 'ewington', 'collie']        []            0   \n",
       "3   ['kimberley', 'halls creek orogen', 'australia']        []            0   \n",
       "4                                                 []        []            0   \n",
       "\n",
       "   reviewed Near Miss Event Key trigger phrase  \n",
       "0      True           False                NaN  \n",
       "1      True           False                NaN  \n",
       "2      True           False                NaN  \n",
       "3      True           False                NaN  \n",
       "4      True           False                NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the training dataset \n",
    "events = pd.read_csv('../events/group_2_labelled.csv')\n",
    "events = events.loc[events['Near Miss Event'].notna(), ]\n",
    "\n",
    "# event_mapper = {val: enum for enum, val in zip(*events.event_id.factorize())}\n",
    "# file_mapper = {val: enum for enum, val in zip(*events.filename.factorize())}\n",
    "\n",
    "events.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bibliography bunting ja 1986, geology of the eastern part of the nabberu basin western australia. west australian geological survey bulletin 131 130p geological survey of western australia 2005. mineral occurrences and exploration potential of the eraheedy area western australia. location: the granite peak project is located about 150km north of wiluna.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.loc[0, 'event_text'] #just to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training data with event text and labels \n",
    "X = events['event_text']\n",
    "ylabels = events['Near Miss Event'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ylabels[0] #just to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# split the dataset into training and validation datasets. \n",
    "# The test set will be the remaining data files that have not been labelled \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, ylabels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering \n",
    "\n",
    "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.\n",
    "\n",
    "\n",
    "2.1 Count Vectors as features\n",
    "\n",
    "2.2 TF-IDF Vectors as features\n",
    "\n",
    "Word level\n",
    "\n",
    "N-Gram level\n",
    "\n",
    "Character level\n",
    "\n",
    "2.3 Word Embeddings as features\n",
    "\n",
    "2.4 Text / NLP based features\n",
    "\n",
    "2.5 Topic Models as features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Count Vectors as features\n",
    "Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a count vectorizer # From Daniel # Could we use utilscharlie???\n",
    "\n",
    "count_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "#tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ff9354a9f21a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create a count vectorizer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'\\w{1,}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# transform the training and validation data using count vectorizer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainDF' is not defined"
     ]
    }
   ],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(X_train['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(X_train)\n",
    "xvalid_count =  count_vect.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building\n",
    "The final step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement following different classifiers for this purpose:\n",
    "\n",
    "1. Linear Classifier \n",
    "2. Naive Bayes Classifier\n",
    "3. Support Vector Machine\n",
    "4. Bagging Models\n",
    "5. Boosting Models\n",
    "6. Shallow Neural Networks\n",
    "7. Deep Neural Networks\n",
    "    * Convolutional Neural Network (CNN)\n",
    "    * Long Short Term Modelr (LSTM)\n",
    "    * Gated Recurrent Unit (GRU)\n",
    "    * Bidirectional RNN\n",
    "    * Recurrent Convolutional Neural Network (RCNN)\n",
    "    * Other Variants of Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Linear Classifier (Logistic Regression Model ) \n",
    "### 3.1. 1. Linear Classfiier on Count Vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(tokenizer=<function spacy_tokenizer at 0x0000022D3F645EE0>)),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Logistic Regression Classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using bag of words(?)\n",
    "pipe = Pipeline([('vectorizer', count_vector), ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance for train set:\n",
      "0    76\n",
      "1    64\n",
      "Name: Near Miss Event, dtype: int64\n",
      "\n",
      "Class balance for test set:\n",
      "1    31\n",
      "0    30\n",
      "Name: Near Miss Event, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f'Class balance for train set:\\n{y_train.value_counts()}\\n')\n",
    "print(f'Class balance for validation set:\\n{y_valid.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7377049180327869\n",
      "Logistic Regression Precision: 0.8947368421052632\n",
      "Logistic Regression Recall: 0.5483870967741935\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_valid)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_valid, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_valid, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_valid, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
